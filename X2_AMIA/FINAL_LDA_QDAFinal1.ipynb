{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpJ7s_SIVu_I"
   },
   "source": [
    "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
    "\n",
    "### Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "### Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "### LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "### Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "### Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TDWOgpJWKQa"
   },
   "source": [
    "## Estructura del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEV8WbiWl6k"
   },
   "source": [
    "## Modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "teF9O9JJmG7Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sDBLvbTtlwzs"
   },
   "outputs": [],
   "source": [
    "class ClassEncoder:\n",
    "  def fit(self, y):\n",
    "    self.names = np.unique(y) #Obtengo los nombres unicos de la matriz y\n",
    "    #print(self.names)\n",
    "    #salida: ['setosa' 'versicolor' 'virginica']\n",
    "    \n",
    "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)} #crea un diccionario, donde asignamos a cada \n",
    "    #nombre de los target, su número con la funcion enumerate. \n",
    "    #print(self.name_to_class)\n",
    "    #salida: {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "    self.fmt = y.dtype #crea un array del mismo tipo de dato que y (str) que usará mas tarde \n",
    "    \n",
    "    # Q1: why is there no need for class_to_name?\n",
    "    \n",
    "    \"\"\" \n",
    "    Al ejecutar la funcion get_iris_data() armamos a la matriz de target con los nombres de las flores para una mejor lectura de \n",
    "    los datos, pero al momento de trabajar con esa matriz necesitamos convertirla a numeros para mejorar la eficiencia del\n",
    "    algoritmo al tener que comparar las clases.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  def _map_reshape(self, f, arr):\n",
    "    \n",
    "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
    "\n",
    "    # Q2: why is reshaping necessary?\n",
    "    \"\"\"\n",
    "    Esta función recibe la matriz de etiquetas y (1,90) y nos devuelve devuelve la matriz de etiquetas (1,90) pero \n",
    "    convertida a nros.\n",
    "    f tiene la información de que valor le corresponde a cada etiqueta, entonces se hace una iteración sobre el array \n",
    "    de etiquetas que es arr.flatten me quedo con el valor numerico. \n",
    "    Luego de esto tenemos como resultado un arreglo con 90 elementos, pero nosotros necesitamos el vector columna\n",
    "    transformado, entoconces realizaomos un reshape para volver a tenerlo.\n",
    "    \n",
    "    La función flatten() se usa para obtener una copia de una matriz dada colapsada en una dimensión, entonces \n",
    "    a la matriz arr que se recibe como parametro se la convierte a una dimensión. Esto se realiza para que sea \n",
    "    genérico el metodo. Al fin y al cabo lo que se quiere hacer es un encoder de los nombres, no cambiar morfología. \n",
    "\n",
    "    \"\"\"\n",
    "  def transform(self, y):\n",
    "    return self._map_reshape(lambda name: self.name_to_class[name], y) \n",
    "\n",
    "  def fit_transform(self, y):\n",
    "    self.fit(y)\n",
    "    return self.transform(y)\n",
    "\n",
    "  def detransform(self, y_hat):# Esta funcion hace la inversa, si le paso un vector con 0,1,2 te devuelve los nombres. \n",
    "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m0KYC8_uSOu4"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    self.encoder = ClassEncoder() \n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    \n",
    "    # Q3: what does bincount do?\n",
    "    \"\"\"\n",
    "    Esta función recibe la matriz y ya codificada con los valores de cada clase y lo que hace con bicount\n",
    "    es contar el numero de ocurrencias de cada valor único en el array. Luego se divide por la longitud de y, \n",
    "    entonces la variable a_priori contiene las frecuencias relativas de cada clase en el dataset.  \n",
    "    Saca basicamente lo que vendría a ser un histograma.\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.log(a_priori)\n",
    "    \n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None): #esta es la función que recibe los datos de train_X train_y\n",
    "    # first encode the classes\n",
    "    y = self.encoder.fit_transform(y) #codifica\n",
    "    \n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    #como a priori esta seteado en none, llamo a la funcion estimar a priori y le envio como parametro la matriz codificada \n",
    "    \n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # check that a_priori has the correct number of classes\n",
    "    #Debemos chequear que tengamos tantas probabilidades a priori como número de clases\n",
    "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    \n",
    "    # Q4: why do we need to do this last, can't we do it first?\n",
    "    \n",
    "    \"\"\"\n",
    "    Sin encoder no se puede indexar con la matriz y porque recibiriamos valores de tipo string\n",
    "    Sin estimar a priori, esta funcion tampoco se podria evaluar len(self.log_a_priori)\n",
    "    \"\"\"\n",
    "##############################----------------inicia predicciones---------------------########################################\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    \n",
    "    #recibe la matriz de caracteristicas X, y guarda el numero de observaciones en m_obs. Usa [1] porque estan en columnas\n",
    "    m_obs = X.shape[1]\n",
    "\n",
    "    #Arma un array vacío del tamaño de las observaciones y del tipo string (aca se van a guardar las predicciones de las clases)\n",
    "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "        \n",
    "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
    "      #llama a la funcion_predict_one y le pasa como parametro la matriz\n",
    "      # de observaciones X (4x90) haciendo este reshape le indicamos que sea de una \n",
    "      #sola columna pero con las mismas filas (4,1) entonces tenemos los 4 datos de una observación \n",
    "        \n",
    "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "\n",
    "    # return prediction as a row vector (matching y) \n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i \n",
    "                  in enumerate(self.log_a_priori)]             \n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IRamFdiGDuSR"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate COMMON  covariance matrix\n",
    "      \n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))#todas las filas tal que pertenezcan a la misma clase. \n",
    "                      for idx in range(len(self.log_a_priori))] #el idx va a tomar los valores 0 1 2 \n",
    "    \n",
    "    # Q5: why not just X[:,y==idx]?\n",
    "    \n",
    "    \"\"\" la variable inv_covs es una lista que contiene tres matrices de covarianza de 4x4 (caracteristicas) en principio, distintas. \n",
    "    Para poder indexar se necesita un vector unidimensional, y el vector y tiene forma de matriz de 90x1. \n",
    "    Por lo cual se utiliza la funcion flatten, que como explicamos anteriormente se usa para obtener una copia\n",
    "    de una matriz dada colapsada en una dimensión.\n",
    "    \"\"\"\n",
    "\n",
    "    # Q6: what does bias=True mean? why not use bias=False?\n",
    "    \n",
    "    \"\"\"\n",
    "    Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA)se asume que cada población \n",
    "    sigue una distribución normal. Y sabemos que para esos casos, el estimador de máxima verosimilitud para la \n",
    "    covarianza de poblaciones que siguen distribucion normal tiene un factor de normalizacion de tipo 1/N, donde N \n",
    "    es el número de observaciones.\n",
    "    \n",
    "    Bias es el sesgo, la normalización predeterminada (bias=False) es 1/(N - 1), (estimación no sesgada).\n",
    "    Si bias=True entonces la normalización es 1/N.\n",
    "\n",
    "    \"\"\"\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    \n",
    "    #Se obtiene la media de las cuatro características para cada clase. Luego el vector de medias es una lista \n",
    "    #de 3 elementos donde cada elemento tiene 4 componentes\n",
    "    \n",
    "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
    "\n",
    "    \"\"\" Se utiliza axis =1 para calcular la media en dirección de los renglones, porque los datos estan traspuestos(4x90). Si se\n",
    "        utilizaría axis=0 la operación se realizaría en dirección a las columnas. \n",
    "   \"\"\"\n",
    "    \n",
    "  def _predict_log_conditional(self, x, class_idx):#recibe la matriz de observaciones redimensionada (4,1) y el indice de cada clase\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    \n",
    "    #Calcula el logaritmo de la probabilidad condicional de X de cada clase\n",
    "    \n",
    "    inv_cov = self.inv_covs[class_idx] \n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS_zoK-gWkRf"
   },
   "source": [
    "## Código para pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m05KrhUDINVs"
   },
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hkXcoldXOqs",
    "outputId": "b07a5027-be83-4c0a-a09e-e4f3a21e4c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (150, 4), Y:(150, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data \n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])  \n",
    "  return X_full, y_full\n",
    "\n",
    "X_full, y_full= get_iris_dataset()\n",
    "\n",
    "\"\"\"\n",
    "    data es un objeto especial de sklearn para guardar dataset y sus atributos\n",
    "\n",
    "    X_full es matriz de 150x4 cada fila es una observación y cada columna una caracteristica (predictor)\n",
    "\n",
    "    data.target reprenta lo que vamos a predecir, el tipo de flor. Es un array np de 150 elementos, con los valores 0,1,2.\n",
    "    \n",
    "    Si aplicamos reshape(-1,1) obtenemos una matriz de (150,1)\n",
    "    En data.target_names tengo los nombres tres tipos de flores ['setosa','versicolor','virginica']\n",
    "    entonces lo que hacemos en y_full es guardar armar una matriz 150x1 con los tres nombres de las flores. \n",
    "    \n",
    "\"\"\"\n",
    "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAk-UQCjKecT",
    "outputId": "ddf4e2f6-1baf-4a51-de72-5ce1d7c03db8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek data matrix\n",
    "X_full[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdzMURX2KVdO",
    "outputId": "66a3cd6b-7dda-4618-b13f-628d113bf7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa']], dtype='<U10')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek target vector\n",
    "y_full[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKP_QmWCIECs",
    "outputId": "36c28bcc-5d33-43e6-f231-3f3bf7b460cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin trasponer (90, 4) (90, 1) (60, 4) (60, 1)\n",
      "Traspuestas (4, 90) (1, 90) (4, 60) (1, 60)\n"
     ]
    }
   ],
   "source": [
    "# preparing data, train - test validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
    "\"\"\"\n",
    "train_test_split es una funcion de sklearn para dividir matrices de datos en dos subconjuntos, datos de entrenamiento y datos \n",
    "de prueba. Con el parametro test=0.4 estamos pidiendo que el 40% del dataset se utilice para la prueba y el 60% para el\n",
    "entrenamiento. Luego con el parametro random_state estamos pidiendo que esa separacion se realice aleatoriamente pero \n",
    "utilizando una semilla para poder obtener nuevamente la misma mezcla.\n",
    "\"\"\"\n",
    "# traspose everything because this model format assumes column vectors\n",
    "\n",
    "train_x = X_train.T\n",
    "train_y = y_train.T\n",
    "test_x = X_test.T\n",
    "test_y = y_test.T\n",
    "\n",
    "print(\"Sin trasponer\",X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(\"Traspuestas\",train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dGIf2TA5SpoT"
   },
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Q30DyLWpTL",
    "outputId": "c113c448-5230-44be-8f85-7a6d3f732d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0500\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yb1V7_yXRfO"
   },
   "source": [
    "# Consigna\n",
    "\n",
    "### Implementación\n",
    "1. Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias?¿Por qué?\n",
    "2. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
    "3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
    "1. *(Opcional)* En `BaseBayesianClassifier._predict_one` se estima la posteriori de cada clase por separado, a pesar de que la cuenta es siempre la misma (cambiando valores de parámetros, pero no dimensiones). Aprovechando el *broadcasting* de NumPy, se puede reemplazar ese list comprehension por un cálculo *tensorizado* donde tanto medias como varianzas (o inversas) estén \"stackeadas\" sobre un tercer eje, permitiendo el cálculo en paralelo de todas las clases con un:\n",
    "`log_posteriori = self.tensor_log_a_priori + self._predict_log_conditionals(x)`. Implementar dicha modificación y mostrar su uso. *Ayuda: los métodos `np.stack` y la documentación del operador [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) son de gran utilidad.*\n",
    "\n",
    "### Preguntas técnicas\n",
    "\n",
    "Responder las 7 preguntas que se encuentran distribuidas a lo largo del código.\n",
    "\n",
    "### Preguntas teóricas\n",
    "\n",
    "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
    "\n",
    "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
    "\n",
    "3. La implementación de QDA estima la probabilidad condicional utilizando 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x que no es exactamente lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respuestas:\n",
    "### <u> Preguntas teoricas </u>\n",
    "\n",
    "#### 1- Derivación de LDA\n",
    "\n",
    "Dado que para LDA $\\Sigma_j == \\Sigma $:\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva. Luego, distribuyendo las matrices resultantes:\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2} x^T \\Sigma^{-1} x + \\frac{1}{2} x^T \\Sigma^{-1} \\mu_j + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
    "$$\n",
    "\n",
    "El primer término de la suma es independiente de la clase j, por lo que se puede incorporar a la constante aditiva.\n",
    "Ahora, si tomamos el segundo término de la suma, usando la propiedad de la transpuesta de un producto de matrices $ (AB)^T = B^T A^T $ y que $ (A^T)^T = A $ , resulta que \n",
    "$$\n",
    "x^T \\Sigma^{-1} \\mu_j = ( \\mu_j^T (\\Sigma^{-1})^T x )^T\n",
    "$$\n",
    "Dado que la matríz $\\Sigma$ es cuadrada y simétrica, luego $\\Sigma ^ -1$ es cuadrada y simétrica, entonces $ (\\Sigma^{-1})^T = (\\Sigma^{-1})$. Por lo tanto, la ecuación para el logaritmo de la probabilidad resulta:\n",
    "$$\n",
    "\\log{f_j(x)} =  \\frac{1}{2} [\\mu_j^T \\Sigma^{-1} x ]^T + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C \n",
    "$$\n",
    "\n",
    "Ahora, viendo las dimensiones de las matrices del primer término, tenemos que $\\mu_j \\epsilon ℝ^{(1xk)}$,  $\\Sigma \\epsilon ℝ^{(kxk)}$ y  $x \\epsilon ℝ^{(kx1)} $, donde k es la cantidad de features del dataset.\n",
    "\n",
    "Por lo tanto $ [\\mu_j^T \\Sigma^{-1} x ] $ es un escalar y la ecuación se puede reducir de la siguiente forma:\n",
    "$$\n",
    "\\log{f_j(x)} =  \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x + \\frac{1}{2} \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
    "$$\n",
    "\n",
    "Lo que resulta que, para LDA:\n",
    "$$\n",
    "\\log{f_j(x)} = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j^T) + C\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### 2- Quadratic vs Linear\n",
    "\n",
    "Se sabe que la expresion a maximizar es\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva. Luego, distribuyendo las matrices resultantes:\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2} x^T \\Sigma_j^{-1} x + \\frac{1}{2} x^T \\Sigma_j^{-1} \\mu_j + \\frac{1}{2} \\mu_j^T \\Sigma_j^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma_j^{-1} \\mu_j + C \n",
    "$$\n",
    "\n",
    "\n",
    "En el caso de LDA El termino $-\\frac{1}{2} x^T \\Sigma^{-1} x $ es cuadratico en x, pero la matriz de covarianza NO depende de la clase (porque por hipotesis se supone que es la misma para todas las clases) por lo cual el termino es igual a todas las clases y puede considerarse como invariable, por lo que tambien puede incorporarse a la variable aditiva. Al incorporarse este termino a la constante aditiva, la ecuacion a maximizar solo tiene una componente lineal en x (como se ve en la expresion final de la pregunta teorica 1)\n",
    "\n",
    "En el caso de QDA sin embargo el termino $-\\frac{1}{2} x^T \\Sigma_j^{-1} x $ es cuadratico en x, y ademas la matriz de covarianza depende de la clase por lo cual el termino entero varia por clase y no puede considerarse constante. Ese termino cuadratico en la ecuacion a maximizar es el que da a QDA su componente cuadratica.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3-  Propiedades de los logaritmos\n",
    "\n",
    "En la parte teórica, se presenta la siguiente fórmula para QDA:\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "Sin embargo, el codigo provisto usa la inversa de matriz de covarianza: `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`\n",
    "\n",
    "Estas dos formas de calcular el resultado son equivalentes ya que, $ |\\Sigma_j| = \\frac{1}{|\\Sigma_j^{-1}|} = (|\\Sigma_j^{-1}|)^{-1} $ y por lo tanto el primer término de la fórmula teórica equivale a $  -\\frac{1}{2}\\log |\\Sigma_j| = +\\frac{1}{2}\\log |\\Sigma_j^{-1}| $\n",
    "\n",
    "En la implementación práctica se prioriza utilizar la variable $ \\Sigma_j^{-1} $ ya que es necesaria para calcular el segundo término y de esta forma se evita tener que guardar el valor tanto de $ \\Sigma_j $ como de $ \\Sigma_j^{-1} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Implementaciones </u>\n",
    "\n",
    "#### 1) Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias? ¿Por qué?\n",
    "\n",
    "No se encontraron diferencias significativas al entrenar un modelo de QDA con una distribución uniforme en comparación con el modelo original provisto.\n",
    "\n",
    "Esto se debe a que, en el caso de no proveer explicitamente una distribución a_priori cuando se llama al método QDA.fit(), la clase QDA computa automáticamente la distribución a_priori como la proporción de cada clase dentro del dataset. \n",
    "Para el caso del dataset \"iris\", el mismo se encuentra perfectamente balanceado, habiendo 50 muestras para cada clase. Por esta razón, el train_test_split() suele devolver dataset de train casi balanceados, por lo que el cálculo automático de QDA.fit() termina computando una distribución \"casi\" uniforme (ya que la proporción de cada clase dentro del dataset es la misma).\n",
    "\n",
    "Aún así, si forzamos a que la distribución \"a priori\" sea uniforme, notamos que pueden existir diferencias respecto al código original aunque no son significativas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0500\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "\n",
    "a_priori_uniforme = np.array([1/3,1/3,1/3])\n",
    "\n",
    "qda.fit(train_x, train_y,a_priori_uniforme)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1ChynN-GXSL5"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate COMMON  covariance matrix\n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) \n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    \n",
    "\n",
    "    total = len(y)  # ejemplo 90\n",
    "    (unique, counts) = np.unique(y, return_counts=True)  # (0, cantidad de ceros) (1, cantidad de 1) (2, cantidad de 2)\n",
    "    self.inv_cov_inbetween = np.zeros(self.inv_covs[0].shape)\n",
    "\n",
    "    #asumo que las matrices de covariaza estan ordenadas por clase. entonces\n",
    "    for index in range(len(counts)):\n",
    "      self.inv_cov_inbetween = self.inv_cov_inbetween + np.multiply( self.inv_covs[index], (counts[index])/total )\n",
    "\n",
    "    print(np.shape(self.inv_cov_inbetween))\n",
    "    self.inv_covs = [self.inv_cov_inbetween for i in range(len(unique))]\n",
    "\n",
    "\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(np.linalg.inv( inv_cov))) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "un_lda = LDA()\n",
    "\n",
    "un_lda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Train (apparent) error is 0.01111111 while test error is 0.05000000\n",
      "LDA Train (apparent) error is 0.01111111 while test error is 0.03333333\n"
     ]
    }
   ],
   "source": [
    "def accuracy_qda(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_acc_q = accuracy_qda(train_y, qda.predict(train_x))\n",
    "test_acc_q = accuracy_qda(test_y, qda.predict(test_x))\n",
    "print(f\"QDA Train (apparent) error is {1-train_acc_q:.8f} while test error is {1-test_acc_q:.8f}\")\n",
    "\n",
    "\n",
    "def accuracy_lda(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_acc_l = accuracy_lda(train_y, un_lda.predict(train_x))\n",
    "test_acc_l = accuracy_lda(test_y, un_lda.predict(test_x))\n",
    "print(f\"LDA Train (apparent) error is {1-train_acc_l:.8f} while test error is {1-test_acc_l:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
    "\n",
    "### Random seed: 6543\n",
    "\n",
    "QDA Train (apparent) error is 0.01111111 while test error is 0.01666667\n",
    "\n",
    "LDA Train (apparent) error is 0.04444444 while test error is 0.0166666\n",
    "\n",
    "### Random seed: 0\n",
    "\n",
    "QDA Train (apparent) error is 0.01111111 while test error is 0.03333333\n",
    "\n",
    "LDA Train (apparent) error is 0.01111111 while test error is 0.08333333\n",
    "\n",
    "### Random seed: 42\n",
    "\n",
    "QDA Train (apparent) error is 0.01111111 while test error is 0.01666667\n",
    "\n",
    "LDA Train (apparent) error is 0.03333333 while test error is 0.01666667\n",
    "\n",
    "### Random seed: 65\n",
    "QDA Train (apparent) error is 0.01111111 while test error is 0.05000000\n",
    "\n",
    "LDA Train (apparent) error is 0.01111111 while test error is 0.03333333\n",
    "\n",
    "\n",
    "Observaciones\n",
    "- El error aparente de QDA en train es independiente del random seed. \n",
    "- El error aparente de LDA en train es dependiente del random seed. \n",
    "Esto puede deberse a que dado que dependiendo como se haga la seleccion de puntos de muestra durante el split, las matrices de covarianza resultantes para cada clase   pueden ser mas o menos distintas y estra mas o menos balanceadas, en cuyo caso la suposicion de una identica matriz de covarianza entre clases obtenidas a travez de un promedio ponderado resulta en que no todas las poblaciones en train pueden clasificarse correctamente mediante la misma. \n",
    "\n",
    "- En test, no hay un claro modelo preferencial, en ocaciones QDA devuelve una mejor prediccion de datos de test, en ocaciones LDA. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AMIA - TP final LDA/QDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "54dee8e2628c7a348348274027baca590104374d7a70a03370ff5ec22cf94c02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
