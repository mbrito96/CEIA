{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "En las siguientes imágenes, se presenta la traslación de un objeto en la capa de entrada de una CNN (el 2 de las imágenes inferiores) y las neuronas que se activan a la salida de dicha CNN para 2 tipos de propiedadedes de las CNN: *Invarianza al desplazamiento (translational invariance)* y *equivariancia al desplazamiento (translational equivariance)*.\n",
    "\n",
    "\n",
    "![a](https://drive.google.com/uc?export=view&id=1buWr91SCZcx4Zx55VLpAf1mCxqVgHgci)\n",
    "\n",
    "Imagen 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![b](https://drive.google.com/uc?export=view&id=1FqUKjutcRL-1Vay0HYY-1tXlISCZFH02)\n",
    "\n",
    "Imagen 2\n",
    "\n",
    "\n",
    "Preguntas:\n",
    "* a) ¿Qué imagen se corresponde con cuál propiedad? \n",
    "* b) ¿Cuál/cuáles de cada capa elemental de una CNN (convolución - activación - pooling) aporta cada propiedad?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "La imagen 1 se corresponde con la propiedad de <i>TRANSLATIONAL EQUIVARIANCE</i>.\n",
    "La imagen 2 se corresponde con la propiedad de <i>TRANSLATIONAL INVARIANCE</i>. <br>\n",
    "\n",
    "### (b)\n",
    "La <i>translational invariance</i> viene dada por la capa de pooling. Esta capa se encarga de reemplazar el output de las capas anteriores (convolucion + activacion) en una ubicación dada por cierto valor representativo de esa zona de pixeles, como puede ser el máximo. Esto hace que aunque los pixeles cambian levemente en una zona, la salida de la capa de pooling se mantendrá igual para la mayoría de las zonas. Estos otorga a la propiedad de invarianza a la translación, ya que no importa exactamente donde se encuentra el objecto a detectar.\n",
    "<br>\n",
    "\n",
    "La <i>translational equivariance </i> viene dada por el concepto de <i>weight sharing</i> dentro de la capa de convolución. Esto consiste en usar el mismo kernel, y por lo tanto los mismos pesos sinápticos, para toda la capa de convolución. De esta forma, el kernel identifca un objeto en una imagen sin importar la posición que tenga dentro de cada imagen.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>Referencia: https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a </i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b49e0765e4a16a08c24abfcabbb0a59b7391929f6c139a67008d70625e2fd730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
